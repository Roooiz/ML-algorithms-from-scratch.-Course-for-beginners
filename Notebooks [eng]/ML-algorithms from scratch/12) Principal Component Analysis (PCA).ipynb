{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Principal Component Analysis**\n",
        "Principal Component Analysis or PCA is an unsupervised learning algorithm used to reduce the dimension and identify the most informative features in the data. Its essence lies in the assumption of the linearity of data relations and their projection onto a subspace of orthogonal vectors in which the variance will be maximal. Such vectors are called the main components and they determine directions of the greatest variability (informativeness) of the data. Alternatively, the essence of PCA can be defined as a linear projection that minimizes the RMS distance between the source points and their projections."
      ],
      "metadata": {
        "id": "OjhUfMJ5v37E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The principle of operation of PCA**\n",
        "Initially, the feature matrix is necessarily centered so that the first main component can correspond to the direction of maximum variation of the data and not just their average value. Usually, finding the main components comes down to two main methods:\n",
        " - **Calculation of eigenvectors and eigenvalues of the covariance matrix of data**. Since the covariance matrix reflects the degree of linear relationship between different variables, the eigenvectors of this matrix will set the directions along which the data variance is maximum, and the eigenvalues will set the magnitude of this variance. The eigenvalue corresponding to the eigenvector characterizes the contribution of this vector to the explanation of the variance of the data and the greater the eigenvalue, the more significant the main component. Usually, only those main components are selected that explain a given level of variance, for example, 95%.\n",
        "\n",
        " - **Calculation of the singular value decomposition of the data matrix**. Singular value decomposition is a way of representing any matrix as a product of three other matrices: the left singular matrix U, the diagonal matrix of singular values S, and the right singular matrix V, where the singular values are the square roots of the eigenvalues of the covariance matrix of the data (this is what data pre-centering is done for in this case), the right singular matrix V it will correspond to the eigenvectors of the covariance matrix of the data, and the left U will be the projection of the original data onto the main components defined by the matrix V. Thus, the singular value decomposition also makes it possible to isolate the main components, but without the need to calculate the covariance matrix. Besides the fact that such a solution is more efficient, it is considered more numerically stable, since it does not require calculating the covariance matrix directly which may be poorly conditioned in the case of a strong correlation of features. Particularly this approach is used in the implementation of scikit-learn, but with some of the features discussed below."
      ],
      "metadata": {
        "id": "8OCN7le0v3vY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SVD-based PCA is constructed as follows:**\n",
        "- 1) data centering occurs first and the number of components is determined at least between the number of samples and features if the number of components has not been specified;\n",
        "- 2) Next, SVD is applied to the centered data matrix;\n",
        "- 3) the svd_flip_vector method is applied to the matrix U which finds the maximum modulo elements in each column of the matrix U, extracts their signs and multiplies the matrix U by these signs to guarantee a deterministic output;\n",
        "- 4) the explained variance for each principal component is calculated as squared corresponding singular values divided by n_samples - 1 and the transformed data is calculated taking into account the number of principal components according to the rule $X_{new} = X \\cdot V = U \\cdot S \\cdot V^T \\cdot V = U \\cdot S$."
      ],
      "metadata": {
        "id": "mppCbZCFw1fY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Additional features of PCA**\n",
        "**The coefficient of explained variance** of each main component, available through the variable *explained_variance_ratio_*, indicates the proportion of variance of the dataset lying along the axis of each main component.\n",
        "\n",
        "**Data recovery** using the *inverse_transform()* method consists in applying the inverse transformation of the PCA projection of the form $X_{recovered} = X_{d-proj} W_d^T$, where $W_d^T$ is a matrix of the first d principal components. It follows that the data will be recovered with losses proportional to the amount of discarded variance of the original data and the average square of the distance between the original and restored data represents a *reconstruction error*.\n",
        "\n",
        "**Incremental PCA**, implemented as the [*IncrementalPCA*](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA) class, allows you to work more efficiently with large datasets by splitting them into mini-packages and storing them in memory piece by piece during training.\n",
        "\n",
        "**Randomized PCA**, set using the svd_solver='randomized' parameter, uses a stochastic algorithm to quickly calculate approximate d principal components and it is based on the assumption that a random projection of data onto a low-dimensional subspace can preserve their structure and properties well, however, this approach may be less accurate.\n",
        "\n",
        "**Kernel PCA**, implemented using the [*KernelPCA*](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html) class, allows you to perform complex non-linear projections using kernel functions. As in the case of SVM, its essence in this case is that the linear boundary of solutions in a multidimensional feature space will correspond to a complex non-linear boundary in the original space."
      ],
      "metadata": {
        "id": "y_dUBdo1yKEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Alternatives to PCA**\n",
        "Despite the fact that the principal component method is one of the most popular dimensionality reduction algorithms there are alternatives that may be more preferable in a number of situations, as well as depending on the type of data:\n",
        "- **LLE (Locally Linear Embedding)** is an algorithm for creating linear combinations of each point from its neighbors, followed by restoring these combinations in a lower dimensional space which allows you to preserve the nonlinear geometry of the data and be useful for some tasks where global properties are less important. On the other hand, this approach has high computational complexity and may be sensitive to noise.\n",
        "- **t-SNE (t-Distributed Stochastic Neighbor Embedding)** is an algorithm that converts similarities between data into probabilities and further tries to minimize the discrepancy between probability distributions in high and low dimensional space. t-SNE is effective in visualizing high-dimensional data, but it can distort the global data structure because it does not take into account linear dependencies, but only their proximity in the original space.\n",
        "- **UMAP (Uniform Manifold Approximation and Projection)** is another algorithm suitable for data visualization which is based on the idea that the data lies on some homogeneous manifold that can be approximated using a neighbor graph. This approach takes into account the global data structure and allows for better adaptation to different types of data and better handling of noise and outliers than t-SNE.\n",
        "- **Autoencoders** is a type of neural networks based on training the encoder to convert input data into a low-dimensional representation, followed by training the decoder to restore the original data from this representation. Autoencoders can also be used for data compression, noise removal, and many other purposes."
      ],
      "metadata": {
        "id": "kQQAKxouym9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Python implementation from scratch**"
      ],
      "metadata": {
        "id": "d0NqRbif1Ukt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris"
      ],
      "metadata": {
        "id": "m0OOS98L16no"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SVDPCA:\n",
        "    def __init__(self, n_components=None):\n",
        "        self.n_components = n_components\n",
        "\n",
        "    @staticmethod\n",
        "    def svd_flip_vector(U):\n",
        "        max_abs_cols_U = np.argmax(np.abs(U), axis=0)\n",
        "        # extract the signs of the max absolute values\n",
        "        signs_U = np.sign(U[max_abs_cols_U, range(U.shape[1])])\n",
        "\n",
        "        return U * signs_U\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "        X_centered = X - X.mean(axis=0)\n",
        "\n",
        "        if self.n_components is None:\n",
        "            self.n_components = min(n_samples, n_features)\n",
        "\n",
        "        U, S, Vt = np.linalg.svd(X_centered)\n",
        "        # flip the eigenvector sign to enforce deterministic output\n",
        "        U_flipped = self.svd_flip_vector(U)\n",
        "\n",
        "        self.explained_variance = (S[:self.n_components] ** 2) / (n_samples - 1)\n",
        "        self.explained_variance_ratio = self.explained_variance / np.sum(self.explained_variance)\n",
        "\n",
        "        # X_new = X * V = U * S * Vt * V = U * S\n",
        "        X_transformed = U_flipped[:, : self.n_components] * S[: self.n_components]\n",
        "\n",
        "        return X_transformed"
      ],
      "metadata": {
        "id": "DO5dvAD_16b3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Uploading a dataset**"
      ],
      "metadata": {
        "id": "tSIQ9NJH2EGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = load_iris(return_X_y=True, as_frame=True)\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7m0fRrsi2FSD",
        "outputId": "1148a141-7959-49ef-eea3-614e2ffc8843"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "0                  5.1               3.5                1.4               0.2\n",
            "1                  4.9               3.0                1.4               0.2\n",
            "2                  4.7               3.2                1.3               0.2\n",
            "3                  4.6               3.1                1.5               0.2\n",
            "4                  5.0               3.6                1.4               0.2\n",
            "..                 ...               ...                ...               ...\n",
            "145                6.7               3.0                5.2               2.3\n",
            "146                6.3               2.5                5.0               1.9\n",
            "147                6.5               3.0                5.2               2.0\n",
            "148                6.2               3.4                5.4               2.3\n",
            "149                5.9               3.0                5.1               1.8\n",
            "\n",
            "[150 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model training and evaluation of the obtained results**\n",
        "The manual implementation showed identical scikit-learn results. As you can see, the first 2 main components explain almost 98% of the variance in the data which allows you to reduce the number of features by half without much loss of information. If the number of features were not 4 but several thousand or millions, then this would significantly reduce the training time of models without significant loss of accuracy (and sometimes with increased accuracy by reducing multicollinearity between features) what makes PCA and its alternatives an excellent addition to other algorithms."
      ],
      "metadata": {
        "id": "73RONIPv2NUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PCA**"
      ],
      "metadata": {
        "id": "haSyFD092g1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = SVDPCA()\n",
        "X_transformed = pca.fit_transform(X)\n",
        "\n",
        "print('transformed data', X_transformed[:10], '', sep='\\n')\n",
        "print('explained_variance', pca.explained_variance)\n",
        "print('explained_variance_ratio', pca.explained_variance_ratio)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q44z3MS2gTK",
        "outputId": "a8bb811f-828f-49fa-cdc3-c8b5174139ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformed data\n",
            "[[-2.68412563e+00  3.19397247e-01 -2.79148276e-02 -2.26243707e-03]\n",
            " [-2.71414169e+00 -1.77001225e-01 -2.10464272e-01 -9.90265503e-02]\n",
            " [-2.88899057e+00 -1.44949426e-01  1.79002563e-02 -1.99683897e-02]\n",
            " [-2.74534286e+00 -3.18298979e-01  3.15593736e-02  7.55758166e-02]\n",
            " [-2.72871654e+00  3.26754513e-01  9.00792406e-02  6.12585926e-02]\n",
            " [-2.28085963e+00  7.41330449e-01  1.68677658e-01  2.42008576e-02]\n",
            " [-2.82053775e+00 -8.94613845e-02  2.57892158e-01  4.81431065e-02]\n",
            " [-2.62614497e+00  1.63384960e-01 -2.18793179e-02  4.52978706e-02]\n",
            " [-2.88638273e+00 -5.78311754e-01  2.07595703e-02  2.67447358e-02]\n",
            " [-2.67275580e+00 -1.13774246e-01 -1.97632725e-01  5.62954013e-02]]\n",
            "\n",
            "explained_variance [4.22824171 0.24267075 0.0782095  0.02383509]\n",
            "explained_variance_ratio [0.92461872 0.05306648 0.01710261 0.00521218]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PCA (scikit-learn)**"
      ],
      "metadata": {
        "id": "KPNSTfkk2noR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sk_pca = PCA()\n",
        "sk_X_transformed = sk_pca.fit_transform(X)\n",
        "\n",
        "print('sk transformed data', sk_X_transformed[:10], '', sep='\\n')\n",
        "print('sk explained_variance', sk_pca.explained_variance_)\n",
        "print('sk explained_variance_ratio_', sk_pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avObFcAI2oZh",
        "outputId": "c47941f6-231d-4969-a521-db26d264f8da"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sk transformed data\n",
            "[[-2.68412563e+00  3.19397247e-01 -2.79148276e-02 -2.26243707e-03]\n",
            " [-2.71414169e+00 -1.77001225e-01 -2.10464272e-01 -9.90265503e-02]\n",
            " [-2.88899057e+00 -1.44949426e-01  1.79002563e-02 -1.99683897e-02]\n",
            " [-2.74534286e+00 -3.18298979e-01  3.15593736e-02  7.55758166e-02]\n",
            " [-2.72871654e+00  3.26754513e-01  9.00792406e-02  6.12585926e-02]\n",
            " [-2.28085963e+00  7.41330449e-01  1.68677658e-01  2.42008576e-02]\n",
            " [-2.82053775e+00 -8.94613845e-02  2.57892158e-01  4.81431065e-02]\n",
            " [-2.62614497e+00  1.63384960e-01 -2.18793179e-02  4.52978706e-02]\n",
            " [-2.88638273e+00 -5.78311754e-01  2.07595703e-02  2.67447358e-02]\n",
            " [-2.67275580e+00 -1.13774246e-01 -1.97632725e-01  5.62954013e-02]]\n",
            "\n",
            "sk explained_variance [4.22824171 0.24267075 0.0782095  0.02383509]\n",
            "sk explained_variance_ratio_ [0.92461872 0.05306648 0.01710261 0.00521218]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pros and cons**\n",
        "Pros:\n",
        "- reducing the dimension while preserving a large amount of information which also allows you to visualize high-dimensional data in two-dimensional or three-dimensional space;\n",
        "- not only allows you to significantly speed up training, but also reduce the overfitting of models in some cases;\n",
        "- can be used to compress data.\n",
        "\n",
        "Cons:\n",
        "- the inevitable loss of some information in the data;\n",
        "- search only for linear dependence in the data (in the classic PCA);\n",
        "- the lack of semantic meaning of the main components due to the difficulty of linking them with real features."
      ],
      "metadata": {
        "id": "sXRg_DP12v-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Additional sources**\n",
        "Papers:\n",
        "- «A Tutorial on Principal Component Analysis», Jonathon Shlens;\n",
        "- «Locally Linear Embedding and its Variants: Tutorial and Survey», Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley;\n",
        "- «Theoretical Foundations of t-SNE for Visualizing High-Dimensional Clustered Data», T. Tony Cai, Rong Ma;\n",
        "- «UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction», Leland McInnes, John Healy, James Melville;\n",
        "- «Deep Autoencoders for Dimensionality Reduction of High-Content Screening Data», Lee Zamparo, Zhaolei Zhang.\n",
        "\n",
        "Documentation:\n",
        "- [PCA description](https://scikit-learn.org/stable/modules/decomposition.html#pca);\n",
        "- [LLE description](https://scikit-learn.org/stable/modules/manifold.html#locally-linear-embedding);\n",
        "- [t-SNE description](https://scikit-learn.org/stable/modules/manifold.html#t-sne);\n",
        "- [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html);\n",
        "- [LLE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html);\n",
        "- [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html);\n",
        "- [UMAP](https://umap-learn.readthedocs.io/en/latest/index.html).\n",
        "\n",
        "Видео:\n",
        "- PCA: [one](https://www.youtube.com/watch?v=FgakZw6K1QQ), [two](https://www.youtube.com/watch?v=fkf4IBRSeEc), [three](https://www.youtube.com/watch?v=IwPzjlBXBlA), [four](https://www.youtube.com/watch?v=WW3ZJHPwvyg);\n",
        "- [LLE](https://www.youtube.com/watch?v=B6kzA1W_4pU);\n",
        "- [t-SNE](https://www.youtube.com/watch?v=NEaUSP4YerM);\n",
        "- [UMAP](https://www.youtube.com/watch?v=eN0wFzBA4Sc);\n",
        "- [autoencoders](https://www.youtube.com/watch?v=FhmpO73ythg)."
      ],
      "metadata": {
        "id": "MlLNPn1S34tc"
      }
    }
  ]
}