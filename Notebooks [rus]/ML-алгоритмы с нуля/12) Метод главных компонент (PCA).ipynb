{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Метод главных компонент**\n",
        "Метод главных компонент (Principal Component Analysis или же PCA) — алгоритм обучения без учителя, используемый для понижения размерности и выявления наиболее информативных признаков в данных. Его суть заключается в предположении о линейности отношений данных и их проекции на подпространство ортогональных векторов, в которых дисперсия будет максимальной. Такие вектора называются главными компонентами и они определяют направления наибольшей изменчивости (информативности) данных. Альтернативно суть PCA можно определить как линейное проецирование, минимизирующее среднеквадратичное расстояние между исходными точками и их проекциями."
      ],
      "metadata": {
        "id": "GEUI9KeXiFd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Принцип работы PCA**\n",
        "Изначально матрица признаков обязательно центрируется, чтобы первая главная компонента могла соответствовать направлению максимальной вариации данных, а не просто их среднему значению. Обычно нахождение главных компонент сводится к двум основным методам:\n",
        " - **Вычисление собственных векторов и собственных значений ковариационной матрицы данных**. Поскольку  ковариационная матрица отражает степень линейной связи между различными переменными, то собственные вектора этой матрицы будут задавать направления, вдоль которых дисперсия данных максимальна, а собственные значения — величину этой дисперсии. Собственное значение, соответствующее собственному вектору, характеризует вклад этого вектора в объяснение дисперсии данных и чем больше собственное значение, тем значимее главная компонента. Обычно отбираются только те главные компоненты, которые объясняют заданный уровень дисперсии, например, 95%.\n",
        "\n",
        " - **Вычисление сингулярного разложения матрицы данных**. Сингулярное разложение — это способ представления любой матрицы в виде произведения трёх других матриц: левой сингулярной матрицы U, диагональной матрицы сингулярных значений S и правой сингулярной матрицы V, где сингулярные значения — это квадратные корни собственных значений ковариационной матрицы данных (именно для этого в данном случае выполняется предварительное центрирование данных), правая сингулярная матрица V будет соответствовать собственным векторам ковариационной матрицы данных, а левая U будет являться проекцией исходных данных на главные компоненты, определённые матрицей V. Таким образом, сингулярное разложение также позволяет выделить главные компоненты, но без необходимости в вычислении ковариационной матрицы. Помимо того, что такое решение более эффективно, оно считается более численно стабильным, поскольку не требует вычисления ковариационной матрицы напрямую, которая может быть плохо обусловлена в случае сильной корреляции признаков. Именно данный подход используется в реализации scikit-learn, но с некоторыми особенностями, рассмотренными ниже.\n",
        "\n",
        "**PCA на основе SVD строится следующим образом:**\n",
        "- 1) сначала происходит центрирование данных, а также определяется число компонент как минимум между числом образцов и признаков в случае, если число компонент не было задано;\n",
        "- 2) далее SVD применяется к центрированной матрице данных;\n",
        "- 3) к матрице U применяется метод svd_flip_vector, который находит максимальные по модулю элементы в каждом столбце матрицы U, извлекает их знаки и умножает матрицу U на эти знаки, чтобы гарантировать детерминированный вывод;\n",
        "- 4) объяснённая дисперсия для каждой главной компоненты вычисляется как возведённые в квадрат соответствующие сингулярные значения, разделённые на n_samples - 1, а преобразованные данные вычисляются с учётом числа главных компонент по правилу $X_{new} = X \\cdot V = U \\cdot S \\cdot V^T \\cdot V = U \\cdot S$.\n"
      ],
      "metadata": {
        "id": "1eGqxmn_8A-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Дополнительные возможности PCA**\n",
        "**Коэффициент объяснённой дисперсии** каждой главной компоненты, доступный через переменную *explained_variance_ratio_*, указывает долю дисперсии датасета, лежащей вдоль оси каждой главной компоненты.\n",
        "\n",
        "**Восстановление данных** с помощью метода *inverse_transform()* заключается в применении обратной трансформации проекции PCA вида $X_{recovered} = X_{d-proj} W_d^T$, где $W_d^T$ — матрица из первых d главных компонент. Из этого следует, что данные будут восстановлены с потерями, пропорциональными количеству отброшенной дисперсии исходных данных, а средний квадрат расстояния между исходными и восстановленными данными представляет собой ошибку восстановления (reconstruction error).\n",
        "\n",
        "**Инкрементный PCA**, реализованный в виде класса *IncrementalPCA*, позволяет работать эффективнее с большими наборами данных за счёт их разбиения на мини-пакеты и поштучном хранении в памяти во время обучения.\n",
        "\n",
        "**Рандомизированный PCA**, устанавливаемый с помощью параметра svd_solver='randomized', использует стохастический алгоритм для быстрого вычисления приближённых d главных компонент и основан на предположении, что случайная проекция данных на низкоразмерное подпространство может хорошо сохранять их структуру и свойства, однако такой подход может быть менее точным.\n",
        "\n",
        "**Ядерный PCA**, реализованный с помощью класса *KernelPCA*, позволяет выполнять сложные нелинейные проекции с использованием ядерных функций. Как и в случае с SVM, его суть в данном случае заключается в том, что линейная граница решений в многомерном пространстве признаков будет соответствовать сложной нелинейной границе в исходном пространстве."
      ],
      "metadata": {
        "id": "bhdxhVqqiT_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Альтернативы PCA**\n",
        "Не смотря на то, что метод главных компонент является одним из самых популярных алгоритмов понижения размерности, существуют альтернативы, которые могут быть более предпочтительными в ряде ситуаций, а также в зависимости от типа данных:\n",
        "- **LLE (Locally Linear Embedding)** — алгоритм создания линейных комбинаций каждой точки из её соседей с последующим восстановлением этих комбинаций в пространстве более низкой размерности, что позволяет сохранить нелинейную геометрию данных и быть полезным для некоторых задач, где глобальные свойства менее важны. С другой стороны, такой подход имеет высокую вычислительную сложность и может быть чувствителен к шуму.\n",
        "- **t-SNE (t-Distributed Stochastic Neighbor Embedding)** — алгоритм, который преобразует сходства между данными в вероятности и в дальнейшем пытается минимизировать расхождение между распределениями вероятностей в пространстве высокой и низкой размерности. t-SNE эффективен при визуализации данных высокой размерности, однако может искажать глобальную структуру данных, поскольку не учитывает линейные зависимости, а лишь их близость в исходном пространстве.\n",
        "- **UMAP (Uniform Manifold Approximation and Projection)** — ещё один алгоритм, подходящий для визуализации данных, который основан на идеи, что данные лежат на некотором однородном многообразии, которое можно аппроксимировать с помощью графа соседей. Такой подход учитывает глобальную структуру данных и позволяет лучше адаптироваться к различным типам данных и лучше справляться с шумом и выбросами, чем t-SNE.\n",
        "- **Autoencoders** — тип нейронных сетей, основанный на обучении кодировщика преобразовывать входные данные в низкоразмерное представление, с последующим обучением декодера восстанавливать исходные данные из этого представления. Autoencoders могут также использоваться для сжатия данных, удаления шума и многих других целей.\n"
      ],
      "metadata": {
        "id": "VMuABOU9iaAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Реализация на Python с нуля**\n"
      ],
      "metadata": {
        "id": "_lBgn_MwyM_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris"
      ],
      "metadata": {
        "id": "IRx6PDrq7PSK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SVDPCA:\n",
        "    def __init__(self, n_components=None):\n",
        "        self.n_components = n_components\n",
        "\n",
        "    @staticmethod\n",
        "    def svd_flip_vector(U):\n",
        "        max_abs_cols_U = np.argmax(np.abs(U), axis=0)\n",
        "        # extract the signs of the max absolute values\n",
        "        signs_U = np.sign(U[max_abs_cols_U, range(U.shape[1])])\n",
        "\n",
        "        return U * signs_U\n",
        "\n",
        "    def fit_transform(self, X):\n",
        "        n_samples, n_features = X.shape\n",
        "        X_centered = X - X.mean(axis=0)\n",
        "\n",
        "        if self.n_components is None:\n",
        "            self.n_components = min(n_samples, n_features)\n",
        "\n",
        "        U, S, Vt = np.linalg.svd(X_centered)\n",
        "        # flip the eigenvector sign to enforce deterministic output\n",
        "        U_flipped = self.svd_flip_vector(U)\n",
        "\n",
        "        self.explained_variance = (S[:self.n_components] ** 2) / (n_samples - 1)\n",
        "        self.explained_variance_ratio = self.explained_variance / np.sum(self.explained_variance)\n",
        "\n",
        "        # X_new = X * V = U * S * Vt * V = U * S\n",
        "        X_transformed = U_flipped[:, : self.n_components] * S[: self.n_components]\n",
        "\n",
        "        return X_transformed"
      ],
      "metadata": {
        "id": "lsf4c0787Nd-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Загрузка датасета**"
      ],
      "metadata": {
        "id": "Ap2qaOBsySp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = load_iris(return_X_y=True, as_frame=True)\n",
        "print(X)"
      ],
      "metadata": {
        "id": "RwZpNYTACi7X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe21d12d-7bbe-490b-c85f-463cb2a725b9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "0                  5.1               3.5                1.4               0.2\n",
            "1                  4.9               3.0                1.4               0.2\n",
            "2                  4.7               3.2                1.3               0.2\n",
            "3                  4.6               3.1                1.5               0.2\n",
            "4                  5.0               3.6                1.4               0.2\n",
            "..                 ...               ...                ...               ...\n",
            "145                6.7               3.0                5.2               2.3\n",
            "146                6.3               2.5                5.0               1.9\n",
            "147                6.5               3.0                5.2               2.0\n",
            "148                6.2               3.4                5.4               2.3\n",
            "149                5.9               3.0                5.1               1.8\n",
            "\n",
            "[150 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Обучение моделей и оценка полученных результатов**\n",
        "Ручная реализация показала идентичные результаты scikit-learn. Как можно заметить, первые 2 главные компоненты объясняют практически 98% дисперсии в данных, что позволяет сократить количество признаков вдвое без особой потери информации. Если бы количество признаков было не 4, а несколько тысяч или миллионов, то это бы позволило существенно сократить время обучения моделей без значительной потери точности (а иногда и с увеличением точности за счёт уменьшения мультиколлинеарности между признаками), что делает PCA и его альтернативы прекрасным дополнением к другим алгоритмам."
      ],
      "metadata": {
        "id": "nc7ILPOKyW_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PCA**"
      ],
      "metadata": {
        "id": "zAlR-QBEglAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = SVDPCA()\n",
        "X_transformed = pca.fit_transform(X)\n",
        "\n",
        "print('transformed data', X_transformed[:10], '', sep='\\n')\n",
        "print('explained_variance', pca.explained_variance)\n",
        "print('explained_variance_ratio', pca.explained_variance_ratio)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4llKYPNLCrys",
        "outputId": "fc82c1ca-5ef5-46d7-e0c4-0891a26446a1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformed data\n",
            "[[-2.68412563e+00  3.19397247e-01 -2.79148276e-02 -2.26243707e-03]\n",
            " [-2.71414169e+00 -1.77001225e-01 -2.10464272e-01 -9.90265503e-02]\n",
            " [-2.88899057e+00 -1.44949426e-01  1.79002563e-02 -1.99683897e-02]\n",
            " [-2.74534286e+00 -3.18298979e-01  3.15593736e-02  7.55758166e-02]\n",
            " [-2.72871654e+00  3.26754513e-01  9.00792406e-02  6.12585926e-02]\n",
            " [-2.28085963e+00  7.41330449e-01  1.68677658e-01  2.42008576e-02]\n",
            " [-2.82053775e+00 -8.94613845e-02  2.57892158e-01  4.81431065e-02]\n",
            " [-2.62614497e+00  1.63384960e-01 -2.18793179e-02  4.52978706e-02]\n",
            " [-2.88638273e+00 -5.78311754e-01  2.07595703e-02  2.67447358e-02]\n",
            " [-2.67275580e+00 -1.13774246e-01 -1.97632725e-01  5.62954013e-02]]\n",
            "\n",
            "explained_variance [4.22824171 0.24267075 0.0782095  0.02383509]\n",
            "explained_variance_ratio [0.92461872 0.05306648 0.01710261 0.00521218]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PCA (scikit-learn)**"
      ],
      "metadata": {
        "id": "m5aG-Cl2goww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sk_pca = PCA()\n",
        "sk_X_transformed = sk_pca.fit_transform(X)\n",
        "\n",
        "print('sk transformed data', sk_X_transformed[:10], '', sep='\\n')\n",
        "print('sk explained_variance', sk_pca.explained_variance_)\n",
        "print('sk explained_variance_ratio_', sk_pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D91ifASgDTbX",
        "outputId": "afa161c5-b856-428d-c5a4-eba6b29204df"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sk transformed data\n",
            "[[-2.68412563e+00  3.19397247e-01 -2.79148276e-02 -2.26243707e-03]\n",
            " [-2.71414169e+00 -1.77001225e-01 -2.10464272e-01 -9.90265503e-02]\n",
            " [-2.88899057e+00 -1.44949426e-01  1.79002563e-02 -1.99683897e-02]\n",
            " [-2.74534286e+00 -3.18298979e-01  3.15593736e-02  7.55758166e-02]\n",
            " [-2.72871654e+00  3.26754513e-01  9.00792406e-02  6.12585926e-02]\n",
            " [-2.28085963e+00  7.41330449e-01  1.68677658e-01  2.42008576e-02]\n",
            " [-2.82053775e+00 -8.94613845e-02  2.57892158e-01  4.81431065e-02]\n",
            " [-2.62614497e+00  1.63384960e-01 -2.18793179e-02  4.52978706e-02]\n",
            " [-2.88638273e+00 -5.78311754e-01  2.07595703e-02  2.67447358e-02]\n",
            " [-2.67275580e+00 -1.13774246e-01 -1.97632725e-01  5.62954013e-02]]\n",
            "\n",
            "sk explained_variance [4.22824171 0.24267075 0.0782095  0.02383509]\n",
            "sk explained_variance_ratio_ [0.92461872 0.05306648 0.01710261 0.00521218]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Преимущества и недостатки**\n",
        "Преимущества:\n",
        "- понижение размерности с сохранением большого количества информации, что также позволяет визуализировать данные высокой размерности в двумерном или трёхмерном пространстве;\n",
        "- не только позволяет значительно ускорить обучение, но и уменьшить переобучение моделей в ряде случаев;\n",
        "- может использоваться для сжатия данных.\n",
        "\n",
        "Недостатки:\n",
        "- неизбежная потеря части информации в данных;\n",
        "- поиск только линейной зависимости в данных (в обычном PCA);\n",
        "- отсутствие смыслового значения главных компонент из-за трудности их связывания с реальными признакам."
      ],
      "metadata": {
        "id": "hFEntYHAksy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Дополнительные источники**\n",
        "Статьи:\n",
        "- «A Tutorial on Principal Component Analysis», Jonathon Shlens;\n",
        "- «Locally Linear Embedding and its Variants: Tutorial and Survey», Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley;\n",
        "- «Theoretical Foundations of t-SNE for Visualizing High-Dimensional Clustered Data», T. Tony Cai, Rong Ma;\n",
        "- «UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction», Leland McInnes, John Healy, James Melville;\n",
        "- «Deep Autoencoders for Dimensionality Reduction of High-Content Screening Data», Lee Zamparo, Zhaolei Zhang.\n",
        "\n",
        "Документация:\n",
        "- [описание PCA](https://scikit-learn.org/stable/modules/decomposition.html#pca);\n",
        "- [описание LLE](https://scikit-learn.org/stable/modules/manifold.html#locally-linear-embedding);\n",
        "- [описание t-SNE](https://scikit-learn.org/stable/modules/manifold.html#t-sne);\n",
        "- [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html);\n",
        "- [LLE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html);\n",
        "- [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html);\n",
        "- [UMAP](https://umap-learn.readthedocs.io/en/latest/index.html).\n",
        "\n",
        "Видео:\n",
        "- PCA: [один](https://www.youtube.com/watch?v=FgakZw6K1QQ), [два](https://www.youtube.com/watch?v=fkf4IBRSeEc), [три](https://www.youtube.com/watch?v=IwPzjlBXBlA), [четыре](https://www.youtube.com/watch?v=WW3ZJHPwvyg);\n",
        "- [LLE](https://www.youtube.com/watch?v=B6kzA1W_4pU);\n",
        "- [t-SNE](https://www.youtube.com/watch?v=NEaUSP4YerM);\n",
        "- [UMAP](https://www.youtube.com/watch?v=eN0wFzBA4Sc);\n",
        "- [autoencoders](https://www.youtube.com/watch?v=FhmpO73ythg)."
      ],
      "metadata": {
        "id": "MEqVtot3k0Vs"
      }
    }
  ]
}